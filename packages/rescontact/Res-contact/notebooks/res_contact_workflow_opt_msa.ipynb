{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Residue–Residue Contact Prediction (ESM2 ± MSA, Local/Online switch)\n",
        "\n",
        "**Purpose**: Train a binary contact predictor from *sequence only*. Ground‑truth contact maps come from PDB/ mmCIF coordinates (Cα–Cα < 8 Å). The network never sees target coordinates as input.\n",
        "\n",
        "### Highlights\n",
        "- **ESM2 embeddings** (Hugging Face) — lightweight model by default.\n",
        "- **MSA features optional**: per‑residue 20‑AA frequencies + entropy and optional MI+APC pair feature.\n",
        "- **Local or Online MSAs**: \n",
        "  - Local: `../data/msa/{train|test}/<pdbid>_<chain>.a3m|.fasta|.fa`\n",
        "  - Online: BLASTp (NCBI) or jackhmmer (EBI) fetched on‑the‑fly and kept **in memory**.\n",
        "- **Memory‑friendly training** for 8‑GB Macs: balanced pair sampling, bilinear scorer (low‑rank), chunked eval.\n",
        "\n",
        "### Folder layout\n",
        "```\n",
        "Res-contact/\n",
        "├─ data/\n",
        "│  ├─ pdb/\n",
        "│  │  ├─ train/   # PDB/mmCIF used for training\n",
        "│  │  └─ test/    # held‑out structures\n",
        "│  └─ msa/\n",
        "│     ├─ train/   # optional local A3M/FASTA (query first)\n",
        "│     └─ test/\n",
        "```\n",
        "\n",
        "**Tip for M‑series Mac (8 GB)**: Keep caps small at first (few files, short max length, small pair subsample). Enable MPS automatically when available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config loaded.\n"
          ]
        }
      ],
      "source": [
        "# =====================\n",
        "# Config (edit freely)\n",
        "# =====================\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# --- Data ---\n",
        "PDB_TRAIN_DIR = Path(\"../data/pdb/train\")\n",
        "PDB_TEST_DIR  = Path(\"../data/pdb/test\")\n",
        "\n",
        "# --- ESM2 backbone ---\n",
        "# Tiny model first; upgrade later if needed (e.g., facebook/esm2_t12_35M_UR50D)\n",
        "ESM2_MODEL_ID = \"facebook/esm2_t6_8M_UR50D\"\n",
        "\n",
        "# --- Contacts ---\n",
        "CA_DIST_THRESH = 8.0  # Å\n",
        "\n",
        "# --- Train/Val/Test split ---\n",
        "VAL_SPLIT = 0.2    # split inside train directory by files\n",
        "SEED = 42\n",
        "\n",
        "# --- Smooth mode knobs (for 8 GB) ---\n",
        "MAX_TRAIN_FILES = 50         # cap number of training files processed\n",
        "MAX_LEN_PER_CHAIN = 600      # truncate long chains (None for no cap)\n",
        "PAIR_SUBSAMPLE_TRAIN = 50_000  # pairs per structure per epoch (balanced sampler overrides semantics)\n",
        "\n",
        "# --- Eval chunking ---\n",
        "EVAL_MAX_PAIRS_PER_STRUCT = 500_000\n",
        "EVAL_BATCH_PAIRS = 100_000\n",
        "\n",
        "# --- MSA FLAGS (this is the requested switch cell) ---\n",
        "USE_LOCAL_MSA  = True            # if A3M/FA exists at ../data/msa/{split}/<pdbid>_<chain>.* use it\n",
        "USE_ONLINE_MSA = True            # if no local file, try online (BLAST or jackhmmer)\n",
        "MSA_PROVIDER   = \"ncbi\"          # \"ncbi\" (BLASTp) or \"jackhmmer\"\n",
        "MSA_MAX_SEQS   = 32              # cap to keep small RAM\n",
        "MSA_EVALUE     = 1e-5\n",
        "MSA_TIMEOUT_S  = 180\n",
        "os.environ.setdefault(\"ENTREZ_EMAIL\", \"ryoji.takahashi@gmail.com\")  # NCBI courtesy\n",
        "os.environ.setdefault(\"MSA_EMAIL\", \"ryoji.takahashi@gmail.com\")     # EBI jackhmmer\n",
        "\n",
        "# --- MSA feature thresholds ---\n",
        "MIN_MSA_DEPTH_FOR_MI = 10   # require at least this many sequences to compute MI\n",
        "MAX_L_FOR_MI        = 800   # don't compute MI if sequence is longer than this\n",
        "\n",
        "# --- Model / training ---\n",
        "EPOCHS = 3                 # small for first pass\n",
        "LR = 1e-3\n",
        "BILINEAR_RANK = 128\n",
        "MI_WEIGHT = 0.5            # scale MI pair feature into the logit (if available)\n",
        "\n",
        "# --- Saving ---\n",
        "SAVE_PATH = Path(\"models/rescontact_best.pt\")\n",
        "RESULTS_DIR = Path(\"results\")\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "SAVE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Config loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/envs/.env_res_contact/lib/python3.11/site-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "device(type='mps')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =====================\n",
        "# Imports & device\n",
        "# =====================\n",
        "import numpy as np\n",
        "import torch\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from Bio.PDB import PDBParser, MMCIFParser, PPBuilder\n",
        "from Bio import SeqIO\n",
        "from Bio import pairwise2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import warnings, io, time, json, requests\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    DEVICE = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MSA I/O — Local files (A3M/FASTA) or Online (BLAST / jackhmmer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _a3m_strip_insertions(s: str) -> str:\n",
        "    return \"\".join([c for c in s if not c.islower()])\n",
        "\n",
        "def read_a3m_as_rows(path: Path, query_seq: str):\n",
        "    with open(path, \"r\") as f:\n",
        "        lines = [ln.strip() for ln in f if ln.strip()]\n",
        "    seqs, cur = [], []\n",
        "    for ln in lines:\n",
        "        if ln.startswith(\">\"):\n",
        "            if cur:\n",
        "                seqs.append(\"\".join(cur)); cur = []\n",
        "        else:\n",
        "            cur.append(ln)\n",
        "    if cur: seqs.append(\"\".join(cur))\n",
        "    seqs = [_a3m_strip_insertions(s) for s in seqs]\n",
        "    if not seqs: return [query_seq]\n",
        "    aligned_query = seqs[0]\n",
        "    L = len(query_seq)\n",
        "    col2qi, qi = [], 0\n",
        "    for c in aligned_query:\n",
        "        if c != \"-\": col2qi.append(qi); qi += 1\n",
        "        else:        col2qi.append(None)\n",
        "    rows = [query_seq]\n",
        "    for aligned in seqs[1:1+MSA_MAX_SEQS]:\n",
        "        row = [\"-\"]*L\n",
        "        for col, aa in enumerate(aligned):\n",
        "            q = col2qi[col]\n",
        "            if q is not None and q < L and aa != \"-\":\n",
        "                row[q] = aa.upper()\n",
        "        rows.append(\"\".join(row))\n",
        "    return rows\n",
        "\n",
        "def read_fasta_as_rows(path: Path, query_seq: str):\n",
        "    records = list(SeqIO.parse(str(path), \"fasta\"))\n",
        "    if not records: return [query_seq]\n",
        "    rows = [query_seq]; L = len(query_seq)\n",
        "    for rec in records[1:1+MSA_MAX_SEQS]:\n",
        "        aln = pairwise2.align.globalms(query_seq, str(rec.seq), 2, -1, -4, -1, one_alignment_only=True)\n",
        "        if not aln: continue\n",
        "        aq, asub, *_ = aln[0]\n",
        "        row = [\"-\"]*L; qi = 0\n",
        "        for cq, cs in zip(aq, asub):\n",
        "            if cq != \"-\":\n",
        "                if cs != \"-\": row[qi] = cs.upper()\n",
        "                qi += 1\n",
        "        rows.append(\"\".join(row))\n",
        "    return rows\n",
        "\n",
        "def read_alignment_as_rows(path: Path, query_seq: str):\n",
        "    ext = path.suffix.lower()\n",
        "    if ext == \".a3m\": return read_a3m_as_rows(path, query_seq)\n",
        "    if ext in (\".fasta\", \".fa\"): return read_fasta_as_rows(path, query_seq)\n",
        "    warnings.warn(f\"Unknown MSA ext {ext}; returning query only\")\n",
        "    return [query_seq]\n",
        "\n",
        "def msa_from_blast_hsps(query_seq: str, hitlist_size: int, evalue: float):\n",
        "    try:\n",
        "        from Bio.Blast import NCBIWWW, NCBIXML\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"BLAST unavailable: {e}\"); return [query_seq]\n",
        "    L = len(query_seq); rows = [query_seq]\n",
        "    try:\n",
        "        handle = NCBIWWW.qblast(\"blastp\", \"nr\", query_seq, hitlist_size=hitlist_size, expect=evalue, format_type=\"XML\")\n",
        "        data = handle.read(); handle.close()\n",
        "        for record in NCBIXML.parse(io.StringIO(data)):\n",
        "            for aln in record.alignments[:hitlist_size]:\n",
        "                if not aln.hsps: continue\n",
        "                hsp = max(aln.hsps, key=lambda h: h.identities)\n",
        "                aq, asub = hsp.query, hsp.sbjct\n",
        "                row = [\"-\"]*L; qi = hsp.query_start - 1\n",
        "                for c_q, c_s in zip(aq, asub):\n",
        "                    if c_q != \"-\":\n",
        "                        if 0 <= qi < L and c_s != \"-\": row[qi] = c_s.upper()\n",
        "                        qi += 1\n",
        "                rows.append(\"\".join(row))\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"BLAST query failed: {e}\")\n",
        "    return rows[:(1+MSA_MAX_SEQS)]\n",
        "\n",
        "def msa_from_jackhmmer(query_seq: str, max_seqs: int, email: str, timeout_s: int = 180):\n",
        "    base = \"https://www.ebi.ac.uk/Tools/hmmer\"\n",
        "    rows = [query_seq]\n",
        "    try:\n",
        "        r = requests.post(f\"{base}/search/jackhmmer\", data={\"seq\": query_seq, \"email\": email}, timeout=20)\n",
        "        r.raise_for_status(); job = r.json()[\"uuid\"]\n",
        "        t0 = time.time()\n",
        "        while True:\n",
        "            s = requests.get(f\"{base}/result/{job}\", timeout=20).json()\n",
        "            if s.get(\"finished\", False): break\n",
        "            if time.time() - t0 > timeout_s: raise TimeoutError(\"jackhmmer timed out\")\n",
        "            time.sleep(3)\n",
        "        sto = requests.get(f\"{base}/result/{job}/aln\", timeout=20).text\n",
        "        seqs = []\n",
        "        for line in sto.splitlines():\n",
        "            if not line or line.startswith((\"#\",\"//\")): continue\n",
        "            toks = line.split()\n",
        "            if len(toks) >= 2: seqs.append(toks[1].replace(\".\", \"-\"))\n",
        "        if not seqs: return rows\n",
        "        L = len(query_seq); aligned_query = seqs[0]\n",
        "        col2qi, qi = [], 0\n",
        "        for c in aligned_query:\n",
        "            if c != \"-\": col2qi.append(qi); qi += 1\n",
        "            else:        col2qi.append(None)\n",
        "        for aligned in seqs[1:1+max_seqs]:\n",
        "            row = [\"-\"]*L\n",
        "            for col, aa in enumerate(aligned):\n",
        "                q = col2qi[col]\n",
        "                if q is not None and q < L and aa != \"-\": row[q] = aa.upper()\n",
        "            rows.append(\"\".join(row))\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"jackhmmer failed: {e}\")\n",
        "    return rows\n",
        "\n",
        "def load_msa_rows(chain_key: str, seq: str, split: str):\n",
        "    # 1) local\n",
        "    if USE_LOCAL_MSA:\n",
        "        base = Path(f\"../data/msa/{split}\")\n",
        "        for ext in (\".a3m\", \".fasta\", \".fa\"):\n",
        "            p = base / f\"{chain_key}{ext}\"\n",
        "            if p.exists(): return read_alignment_as_rows(p, seq)\n",
        "    # 2) online\n",
        "    if USE_ONLINE_MSA:\n",
        "        if MSA_PROVIDER == \"ncbi\":\n",
        "            return msa_from_blast_hsps(seq, hitlist_size=MSA_MAX_SEQS, evalue=MSA_EVALUE)\n",
        "        elif MSA_PROVIDER == \"jackhmmer\":\n",
        "            return msa_from_jackhmmer(seq, max_seqs=MSA_MAX_SEQS, email=os.environ.get(\"MSA_EMAIL\",\"\"), timeout_s=MSA_TIMEOUT_S)\n",
        "    # 3) fallback\n",
        "    return [seq]\n",
        "\n",
        "AA = \"ACDEFGHIKLMNPQRSTVWY\"\n",
        "AA_IDX = {a:i for i,a in enumerate(AA)}\n",
        "\n",
        "def msa_1d_features(rows):\n",
        "    L = len(rows[0])\n",
        "    if len(rows) <= 1: return np.zeros((L,21), dtype=np.float32)\n",
        "    depth = len(rows); freq = np.zeros((L,20), dtype=np.float32)\n",
        "    for r in rows:\n",
        "        for i, aa in enumerate(r):\n",
        "            if aa in AA_IDX: freq[i, AA_IDX[aa]] += 1\n",
        "    freq /= float(depth)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        ent = -np.sum(freq * np.where(freq>0, np.log(freq+1e-12), 0.0), axis=1)\n",
        "    return np.concatenate([freq, ent[:,None].astype(np.float32)], axis=1)\n",
        "\n",
        "def _mi_from_counts(C):\n",
        "    P = C / (np.sum(C) + 1e-9)\n",
        "    pi = P.sum(1, keepdims=True); pj = P.sum(0, keepdims=True)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        MI = np.nansum(P * (np.log(P+1e-12) - np.log(pi+1e-12) - np.log(pj+1e-12)))\n",
        "    return float(MI)\n",
        "\n",
        "def msa_pair_mi_apc(rows):\n",
        "    L = len(rows[0]); depth = len(rows)\n",
        "    if depth < MIN_MSA_DEPTH_FOR_MI or L > MAX_L_FOR_MI: return None\n",
        "    X = np.full((depth, L), -1, dtype=np.int16)\n",
        "    for r, row in enumerate(rows):\n",
        "        for i, aa in enumerate(row):\n",
        "            X[r,i] = AA_IDX.get(aa, -1)\n",
        "    MI = np.zeros((L,L), dtype=np.float32)\n",
        "    for i in range(L):\n",
        "        for j in range(i+1, L):\n",
        "            C = np.zeros((20,20), dtype=np.float32)\n",
        "            for r in range(depth):\n",
        "                a, b = X[r,i], X[r,j]\n",
        "                if a>=0 and b>=0: C[a,b] += 1\n",
        "            val = _mi_from_counts(C)\n",
        "            MI[i,j] = MI[j,i] = val\n",
        "    row_mean = MI.mean(axis=1, keepdims=True)\n",
        "    col_mean = MI.mean(axis=0, keepdims=True)\n",
        "    glob = MI.mean()\n",
        "    MI_apc = MI - row_mean*col_mean/(glob+1e-9)\n",
        "    np.fill_diagonal(MI_apc, 0.0)\n",
        "    return MI_apc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Structure parsing → ATOM sequences & CA contacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _parser_for(path: Path):\n",
        "    ext = path.suffix.lower()\n",
        "    if ext == \".pdb\": return PDBParser(QUIET=True)\n",
        "    if ext in (\".cif\", \".mmcif\"): return MMCIFParser(QUIET=True)\n",
        "    raise ValueError(f\"Unsupported structure format: {path}\")\n",
        "\n",
        "def load_structure(path: Path):\n",
        "    return _parser_for(path).get_structure(path.stem, str(path))\n",
        "\n",
        "def extract_atom_seq_by_chain(struct):\n",
        "    seqs = {}; ppb = PPBuilder(); model = next(iter(struct))\n",
        "    for chain in model:\n",
        "        polypeps = list(ppb.build_peptides(chain, aa_only=False))\n",
        "        if not polypeps: continue\n",
        "        s = \"\".join([str(pp.get_sequence()) for pp in polypeps])\n",
        "        if s: seqs[chain.id] = s\n",
        "    return seqs\n",
        "\n",
        "def extract_ca_coords_by_chain(struct):\n",
        "    ppb = PPBuilder(); chain_coords = {}; model = next(iter(struct))\n",
        "    for chain in model:\n",
        "        polypeps = ppb.build_peptides(chain)\n",
        "        if not polypeps: continue\n",
        "        coords = []; offset = 0\n",
        "        for pp in polypeps:\n",
        "            for i, res in enumerate(pp):\n",
        "                if \"CA\" in res: coords.append((offset+i, res[\"CA\"].coord.copy()))\n",
        "            offset += len(pp)\n",
        "        if coords: chain_coords[chain.id] = coords\n",
        "    return chain_coords\n",
        "\n",
        "def contact_map_from_coords(coords, L, thresh):\n",
        "    has = np.zeros(L, dtype=bool); xyz = np.zeros((L,3), dtype=np.float32)\n",
        "    for i,c in coords:\n",
        "        if i < L: has[i] = True; xyz[i] = c\n",
        "    idx = np.where(has)[0]\n",
        "    C = np.zeros((L,L), dtype=bool)\n",
        "    if len(idx) > 0:\n",
        "        sub = xyz[idx]\n",
        "        d = np.sqrt(((sub[:,None,:]-sub[None,:,:])**2).sum(-1))\n",
        "        c = d < thresh\n",
        "        for a, ia in enumerate(idx):\n",
        "            for b, ib in enumerate(idx):\n",
        "                C[ia, ib] = c[a,b]\n",
        "    V = np.zeros((L,L), dtype=bool)\n",
        "    for i in idx:\n",
        "        for j in idx:\n",
        "            if i!=j: V[i,j] = True\n",
        "    return C, V, has\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Embeddings (ESM2 via HF) with in‑memory cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/envs/.env_res_contact/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/opt/miniconda3/envs/.env_res_contact/lib/python3.11/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/miniconda3/envs/.env_res_contact/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <EB3FF92A-5EB1-3EE8-AF8B-5923C1265422> /opt/miniconda3/envs/.env_res_contact/lib/python3.11/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/opt/miniconda3/envs/.env_res_contact/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/miniconda3/envs/.env_res_contact/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/miniconda3/envs/.env_res_contact/lib/python3.11/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/miniconda3/envs/.env_res_contact/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedder: EsmModel\n"
          ]
        }
      ],
      "source": [
        "class FallbackEmbedder(torch.nn.Module):\n",
        "    def __init__(self, dim=64, vocab=26):\n",
        "        super().__init__(); self.emb = torch.nn.Embedding(vocab, dim)\n",
        "        torch.nn.init.xavier_uniform_(self.emb.weight)\n",
        "    def forward(self, seq: str):\n",
        "        idx = torch.tensor([(ord(c)%26) for c in seq], dtype=torch.long, device=DEVICE)\n",
        "        return self.emb(idx)\n",
        "\n",
        "def try_load_esm2(model_id: str):\n",
        "    try:\n",
        "        from transformers import AutoModel, AutoTokenizer\n",
        "        tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "        mdl = AutoModel.from_pretrained(model_id, trust_remote_code=True).to(DEVICE).eval()\n",
        "        return tok, mdl\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Could not load {model_id}: {e}; using fallback embedder.\")\n",
        "        return None, FallbackEmbedder().to(DEVICE).eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def embed_sequence(seq: str, tokenizer, model):\n",
        "    if tokenizer is None: return model(seq)\n",
        "    tokens = tokenizer(seq, return_tensors=\"pt\", add_special_tokens=True)\n",
        "    tokens = {k: v.to(DEVICE) for k,v in tokens.items()}\n",
        "    out = model(**tokens)\n",
        "    H = out.last_hidden_state[0]\n",
        "    if H.shape[0] >= len(seq)+2: H = H[1:1+len(seq)]\n",
        "    return H.detach()\n",
        "\n",
        "tokenizer, esm2_model = try_load_esm2(ESM2_MODEL_ID)\n",
        "print(\"Embedder:\", type(esm2_model).__name__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Build per‑structure tensors (concat chains)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "EMBED_CACHE = {}\n",
        "\n",
        "def process_structure(path: Path, split: str):\n",
        "    try:\n",
        "        s = load_structure(path)\n",
        "        chain_seqs = extract_atom_seq_by_chain(s)\n",
        "        chain_coords = extract_ca_coords_by_chain(s)\n",
        "        if not chain_seqs: return None\n",
        "\n",
        "        H_blocks = []; contact_blocks=[]; valid_blocks=[]; mi_tiles=[]; Ls=[]; chain_ids=[]; seqs=[]\n",
        "        for cid in sorted(chain_seqs.keys()):\n",
        "            seq = chain_seqs[cid]\n",
        "            if MAX_LEN_PER_CHAIN is not None and len(seq) > MAX_LEN_PER_CHAIN:\n",
        "                seq = seq[:MAX_LEN_PER_CHAIN]\n",
        "            L = len(seq)\n",
        "            coords = [(i,c) for (i,c) in chain_coords.get(cid, []) if i < L]\n",
        "            if L==0 or len(coords)<2: continue\n",
        "\n",
        "            key = f\"{path.stem}_{cid}_{L}\"\n",
        "            if key in EMBED_CACHE: H = EMBED_CACHE[key]\n",
        "            else:\n",
        "                H = embed_sequence(seq, tokenizer, esm2_model)\n",
        "                EMBED_CACHE[key] = H\n",
        "\n",
        "            rows = load_msa_rows(f\"{path.stem}_{cid}\", seq, split)\n",
        "            feat1d = msa_1d_features(rows)  # [L,21]\n",
        "            H_aug = torch.cat([H, torch.from_numpy(feat1d).to(H.device)], dim=1)\n",
        "\n",
        "            C, V, has = contact_map_from_coords(coords, L, CA_DIST_THRESH)\n",
        "            MI = msa_pair_mi_apc(rows)\n",
        "\n",
        "            H_blocks.append(H_aug); contact_blocks.append(C); valid_blocks.append(V); mi_tiles.append(MI)\n",
        "            Ls.append(L); seqs.append(seq); chain_ids.extend([cid]*L)\n",
        "\n",
        "        if not H_blocks: return None\n",
        "        H_full = torch.cat(H_blocks, dim=0); Ltot = sum(Ls)\n",
        "        C_full = np.zeros((Ltot,Ltot), dtype=bool); V_full = np.zeros((Ltot,Ltot), dtype=bool)\n",
        "        MI_full = None\n",
        "        if any(m is not None for m in mi_tiles): MI_full = np.zeros((Ltot,Ltot), dtype=np.float32)\n",
        "        ci = 0\n",
        "        for k, L in enumerate(Ls):\n",
        "            cj = ci+L\n",
        "            C_full[ci:cj, ci:cj] = contact_blocks[k]\n",
        "            V_full[ci:cj, ci:cj] = valid_blocks[k]\n",
        "            if MI_full is not None and mi_tiles[k] is not None:\n",
        "                MI_full[ci:cj, ci:cj] = mi_tiles[k]\n",
        "            ci = cj\n",
        "\n",
        "        uniq = {c:i for i,c in enumerate(sorted(set(chain_ids)))}\n",
        "        chain_ids_arr = np.array([uniq[c] for c in chain_ids], dtype=np.int64)\n",
        "        return dict(seq=\"\".join(seqs), chain_ids=chain_ids_arr, contact=C_full, valid_pair=V_full,\n",
        "                    H=H_full.to(DEVICE), MI=MI_full, pdb_id=path.stem, path=str(path))\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error processing {path}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model: Low‑rank Bilinear scorer (+ optional MI term)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BilinearScorer(torch.nn.Module):\n",
        "    def __init__(self, d: int, rank: int = 128, mi_weight: float = MI_WEIGHT):\n",
        "        super().__init__()\n",
        "        self.U = torch.nn.Linear(d, rank, bias=False)\n",
        "        self.V = torch.nn.Linear(d, rank, bias=False)\n",
        "        self.bias = torch.nn.Parameter(torch.zeros(1))\n",
        "        self.mi_weight = mi_weight\n",
        "    def forward_pairs(self, H: torch.Tensor, idx: np.ndarray, MI: Optional[np.ndarray] = None):\n",
        "        i = torch.as_tensor(idx[:,0], device=H.device)\n",
        "        j = torch.as_tensor(idx[:,1], device=H.device)\n",
        "        A = self.U(H); B = self.V(H)\n",
        "        logits = (A[i] * B[j]).sum(-1) + self.bias\n",
        "        if MI is not None:\n",
        "            mi_val = torch.from_numpy(MI[idx[:,0], idx[:,1]]).to(logits.device)\n",
        "            logits = logits + self.mi_weight * mi_val\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training utils: balanced pair sampling, batching & evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_train_pairs_balanced(S, max_pos: int = 5000, neg_ratio: int = 3):\n",
        "    C = S[\"contact\"]; V = S[\"valid_pair\"]; L = C.shape[0]\n",
        "    iu, ju = np.triu_indices(L, k=1)\n",
        "    pos_mask = C[iu, ju] & V[iu, ju]\n",
        "    neg_mask = (~C[iu, ju]) & V[iu, ju]\n",
        "    pos_idx = np.stack([iu[pos_mask], ju[pos_mask]], axis=1)\n",
        "    neg_idx = np.stack([iu[neg_mask], ju[neg_mask]], axis=1)\n",
        "    if len(pos_idx)==0: return np.empty((0,2), dtype=int), np.empty((0,), dtype=np.float32)\n",
        "    if max_pos is not None and len(pos_idx) > max_pos:\n",
        "        sel = np.random.choice(len(pos_idx), size=max_pos, replace=False)\n",
        "        pos_idx = pos_idx[sel]\n",
        "    m = min(len(neg_idx), len(pos_idx)*neg_ratio)\n",
        "    if m>0:\n",
        "        sel = np.random.choice(len(neg_idx), size=m, replace=False)\n",
        "        neg_idx = neg_idx[sel]\n",
        "    else:\n",
        "        neg_idx = np.empty((0,2), dtype=int)\n",
        "    pairs = np.vstack([pos_idx, neg_idx])\n",
        "    labels = np.hstack([np.ones(len(pos_idx), dtype=np.float32), np.zeros(len(neg_idx), dtype=np.float32)])\n",
        "    return pairs, labels\n",
        "\n",
        "def collect_struct_files(folder: Path):\n",
        "    files = []\n",
        "    for pat in (\"*.pdb\",\"*.PDB\",\"*.cif\",\"*.CIF\",\"*.mmcif\",\"*.MMCIF\"):\n",
        "        files += list(folder.glob(pat))\n",
        "    return sorted(files)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, structs):\n",
        "    model.eval()\n",
        "    all_scores, all_labels = [], []\n",
        "    per_pdb = {}\n",
        "    for S in structs:\n",
        "        if S is None: continue\n",
        "        H = S[\"H\"]; C = S[\"contact\"]; V = S[\"valid_pair\"]; MI = S[\"MI\"]\n",
        "        iu, ju = np.triu_indices(C.shape[0], k=1)\n",
        "        mask = V[iu, ju]\n",
        "        iu, ju = iu[mask], ju[mask]\n",
        "        idx = np.stack([iu, ju], axis=1)\n",
        "        if len(idx)==0: continue\n",
        "        probs_list, labels_list = [], []\n",
        "        for s in range(0, len(idx), EVAL_BATCH_PAIRS):\n",
        "            sl = idx[s:s+EVAL_BATCH_PAIRS]\n",
        "            logits = model.forward_pairs(H, sl, MI)\n",
        "            probs_list.append(torch.sigmoid(logits).detach().cpu().numpy())\n",
        "            labels_list.append(C[sl[:,0], sl[:,1]].astype(int))\n",
        "        probs = np.concatenate(probs_list); labels = np.concatenate(labels_list)\n",
        "        try:\n",
        "            ap = average_precision_score(labels, probs); roc = roc_auc_score(labels, probs)\n",
        "        except Exception:\n",
        "            ap, roc = float('nan'), float('nan')\n",
        "        L = len(S[\"seq\"]); order = np.argsort(-probs)\n",
        "        def p_at(k): k = max(1, min(k, len(order))); return labels[order[:k]].mean()\n",
        "        per_pdb[S[\"pdb_id\"]] = dict(pr_auc=ap, roc_auc=roc, p_at_L=p_at(L), p_at_L2=p_at(L//2), p_at_L5=p_at(max(1,L//5)))\n",
        "        all_scores.append(probs); all_labels.append(labels)\n",
        "    if not all_scores: return dict(global_pr_auc=float('nan'), global_roc_auc=float('nan'), per_pdb=per_pdb)\n",
        "    scores = np.concatenate(all_scores); labels = np.concatenate(all_labels)\n",
        "    try:\n",
        "        g_ap = average_precision_score(labels, scores); g_roc = roc_auc_score(labels, scores)\n",
        "    except Exception:\n",
        "        g_ap, g_roc = float('nan'), float('nan')\n",
        "    return dict(global_pr_auc=g_ap, global_roc_auc=g_roc, per_pdb=per_pdb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train loop (balanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, structs, lr=LR):\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    bce = torch.nn.BCEWithLogitsLoss()\n",
        "    model.train(); total = 0.0\n",
        "    for S in structs:\n",
        "        if S is None: continue\n",
        "        H = S[\"H\"]; MI = S[\"MI\"]\n",
        "        pairs, y = build_train_pairs_balanced(S, max_pos=PAIR_SUBSAMPLE_TRAIN, neg_ratio=3)\n",
        "        if len(pairs)==0: continue\n",
        "        for s in range(0, len(pairs), EVAL_BATCH_PAIRS):\n",
        "            sl = pairs[s:s+EVAL_BATCH_PAIRS]\n",
        "            logits = model.forward_pairs(H, sl, MI)\n",
        "            yy = torch.from_numpy(y[s:s+EVAL_BATCH_PAIRS]).to(logits.device)\n",
        "            loss = bce(logits, yy)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            total += float(loss.detach().cpu())\n",
        "    return total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run: split train/val, train model, evaluate on test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 15000 train files, 500 test files.\n",
            "Capped training files to 50\n",
            "Split: 40 train / 10 val\n",
            "Processing train...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 8/40 [22:28<1:17:52, 146.01s/it]/opt/miniconda3/envs/.env_res_contact/lib/python3.11/site-packages/Bio/PDB/Polypeptide.py:322: UserWarning: Assuming residue CA is an unknown modified amino acid\n",
            "  warnings.warn(\n",
            "/opt/miniconda3/envs/.env_res_contact/lib/python3.11/site-packages/Bio/PDB/Polypeptide.py:322: UserWarning: Assuming residue ETA is an unknown modified amino acid\n",
            "  warnings.warn(\n",
            " 55%|█████▌    | 22/40 [1:04:30<35:54, 119.69s/it]"
          ]
        }
      ],
      "source": [
        "train_files_all = collect_struct_files(PDB_TRAIN_DIR)\n",
        "test_files = collect_struct_files(PDB_TEST_DIR)\n",
        "print(f\"Found {len(train_files_all)} train files, {len(test_files)} test files.\")\n",
        "if MAX_TRAIN_FILES is not None and len(train_files_all) > MAX_TRAIN_FILES:\n",
        "    train_files_all = train_files_all[:MAX_TRAIN_FILES]\n",
        "    print(f\"Capped training files to {len(train_files_all)}\")\n",
        "\n",
        "train_files, val_files = train_test_split(train_files_all, test_size=VAL_SPLIT, random_state=SEED)\n",
        "print(f\"Split: {len(train_files)} train / {len(val_files)} val\")\n",
        "\n",
        "print(\"Processing train...\")\n",
        "train_structs = [process_structure(p, split=\"train\") for p in tqdm(train_files)]\n",
        "train_structs = [s for s in train_structs if s is not None]\n",
        "print(\"Processing val...\")\n",
        "val_structs = [process_structure(p, split=\"train\") for p in tqdm(val_files)]\n",
        "val_structs = [s for s in val_structs if s is not None]\n",
        "\n",
        "if not train_structs:\n",
        "    raise RuntimeError(\"No valid train structures parsed. Check your data paths.\")\n",
        "\n",
        "d_in = train_structs[0][\"H\"].shape[-1]\n",
        "model = BilinearScorer(d=d_in, rank=BILINEAR_RANK, mi_weight=MI_WEIGHT).to(DEVICE)\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    loss = train_epoch(model, train_structs, lr=LR)\n",
        "    val_scores = evaluate_model(model, val_structs)\n",
        "    print(f\"epoch {ep}  loss={loss:.3f}  val PR-AUC={val_scores['global_pr_auc']:.4f}  ROC-AUC={val_scores['global_roc_auc']:.4f}\")\n",
        "\n",
        "torch.save({\"state_dict\": model.state_dict(), \"cfg\": dict(d_in=d_in, rank=BILINEAR_RANK, mi_weight=MI_WEIGHT, lr=LR, epochs=EPOCHS)}, str(SAVE_PATH))\n",
        "print(\"Saved model to\", SAVE_PATH)\n",
        "\n",
        "print(\"Processing test...\")\n",
        "test_structs = [process_structure(p, split=\"test\") for p in tqdm(test_files)]\n",
        "test_structs = [s for s in test_structs if s is not None]\n",
        "test_scores = evaluate_model(model, test_structs)\n",
        "print(\"Test: PR-AUC={:.4f}, ROC-AUC={:.4f}\".format(test_scores['global_pr_auc'], test_scores['global_roc_auc']))\n",
        "\n",
        "df = pd.DataFrame.from_dict(test_scores[\"per_pdb\"], orient=\"index\").reset_index().rename(columns={\"index\":\"pdb_id\"})\n",
        "df.sort_values(\"pr_auc\", ascending=False, inplace=True)\n",
        "out_csv = RESULTS_DIR / \"test_metrics.csv\"\n",
        "df.to_csv(out_csv, index=False)\n",
        "print(\"Wrote\", out_csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Notes & Reporting\n",
        "- **Method**: ESM2 per‑residue embeddings concatenated with MSA 1D features; low‑rank bilinear pair scorer; optional MI+APC term.\n",
        "- **Data steps**: ATOM polypeptides (PPBuilder) → per‑chain sequences; CA contacts @ 8Å; concat chains block‑diagonally; balanced pos/neg sampling.\n",
        "- **Hyperparams**: see Config cell.\n",
        "- **Metrics**: PR‑AUC, ROC‑AUC, Precision@L/L2/L5 per PDB and global.\n",
        "- **MSA switch**: flip `USE_LOCAL_MSA`/`USE_ONLINE_MSA`/`MSA_PROVIDER` in Config — the rest of the pipeline stays unchanged.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".env_res_contact",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
