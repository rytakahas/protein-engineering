{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9db798",
   "metadata": {},
   "source": [
    "# Residue Contact Prediction — Fixed (FINAL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44810159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Residue Contact Prediction — Fixed (FINAL2)\n",
    "# - Forces float32 end-to-end (H, logits, labels)\n",
    "# - Embedding cache loaded as float32 (prevents fp16 leakage into Linear)\n",
    "# - Robust forward_pairs (idx as np or torch; empty-safe)\n",
    "# - Pairs/labels yielded on DEVICE; train loop recasts H->float32 on DEVICE\n",
    "# - BCEWithLogitsLoss target/shape alignment before loss\n",
    "# - Dummy path if no PDBs (Biopython optional)\n",
    "\n",
    "import os, gc, hashlib, random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- Optional Biopython (guarded) ----\n",
    "try:\n",
    "    from Bio.PDB import PDBParser, MMCIFParser, PPBuilder\n",
    "    BIO_AVAILABLE = True\n",
    "except Exception:\n",
    "    BIO_AVAILABLE = False\n",
    "    PDBParser = MMCIFParser = PPBuilder = None\n",
    "\n",
    "# ---- Config ----\n",
    "PDB_TRAIN_DIR = Path(\"../data/pdb/train\")\n",
    "PDB_TEST_DIR  = Path(\"../data/pdb/test\")\n",
    "CACHE_DIR = Path(\"./cache/embeds\"); CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_ID = \"facebook/esm2_t6_8M_UR50D\"\n",
    "CA_DIST_THRESH = 8.0\n",
    "\n",
    "MAX_TRAIN_FILES = 200\n",
    "MAX_LEN_PER_CHAIN = 600\n",
    "VAL_SPLIT = 0.2\n",
    "SEED = 42\n",
    "\n",
    "MAX_POS_PER_STRUCT = 5000\n",
    "NEG_RATIO = 3\n",
    "RANK = 128\n",
    "LR = 1e-3\n",
    "EPOCHS = 5\n",
    "PAIRS_BATCH = 50000\n",
    "EVAL_CHUNK = 200000\n",
    "\n",
    "SAVE_DIR = Path(\"./models\"); SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_PATH = SAVE_DIR / \"rescontact_bilin.pt\"\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "torch.set_default_dtype(torch.float32)  # ensure default ops in fp32\n",
    "\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "\n",
    "# ---- ESM2 loader + fallback + cache ----\n",
    "def _chain_cache_key(seq: str, model_id: str) -> str:\n",
    "    return hashlib.sha1((seq + \"|\" + model_id).encode()).hexdigest()[:16] + \".npz\"\n",
    "\n",
    "def _cache_path_for(seq: str, model_id: str) -> Path:\n",
    "    return CACHE_DIR / _chain_cache_key(seq, model_id)\n",
    "\n",
    "def try_load_esm2(model_id: str):\n",
    "    try:\n",
    "        from transformers import AutoModel, AutoTokenizer\n",
    "        tok = AutoTokenizer.from_pretrained(model_id, use_fast=True, local_files_only=False)\n",
    "        mdl = AutoModel.from_pretrained(model_id, trust_remote_code=True, local_files_only=False)\n",
    "        mdl.to(DEVICE).eval()\n",
    "        return tok, mdl\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] Could not load {model_id}: {e}\\nUsing fallback random embedder.\")\n",
    "        class Fallback(torch.nn.Module):\n",
    "            def __init__(self, dim=64, vocab=26):\n",
    "                super().__init__()\n",
    "                self.emb = torch.nn.Embedding(vocab, dim)\n",
    "                torch.nn.init.xavier_uniform_(self.emb.weight)\n",
    "            def forward(self, s: str):\n",
    "                idx = torch.tensor([(ord(c) % 26) for c in s], dtype=torch.long, device=DEVICE)\n",
    "                return self.emb(idx)\n",
    "        return None, Fallback().to(DEVICE).eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_sequence(seq: str, tokenizer, model) -> torch.Tensor:\n",
    "    cp = _cache_path_for(seq, MODEL_ID)\n",
    "    if cp.exists():\n",
    "        z = np.load(cp)\n",
    "        # FORCE float32 from cache to avoid half precision going into Linear\n",
    "        return torch.from_numpy(z[\"emb\"]).to(DEVICE, dtype=torch.float32)\n",
    "    if tokenizer is None:\n",
    "        H = model(seq)  # fallback produces float32\n",
    "    else:\n",
    "        toks = tokenizer(seq, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        toks = {k: v.to(DEVICE) for k,v in toks.items()}\n",
    "        out = model(**toks)\n",
    "        H = out.last_hidden_state[0]\n",
    "        if H.shape[0] >= len(seq)+2:\n",
    "            H = H[1:1+len(seq)]\n",
    "    # Save fp16 to disk (small), but return fp32 for training\n",
    "    H16 = H.detach().to(torch.float16).cpu().numpy()\n",
    "    np.savez_compressed(cp, emb=H16)\n",
    "    return H.detach().to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "# ---- PDB parsing & labels ----\n",
    "def _parser_for(path: Path):\n",
    "    if not BIO_AVAILABLE:\n",
    "        raise RuntimeError(\"Biopython not available; cannot parse PDB/mmCIF. Use dummy mode.\")\n",
    "    ext = path.suffix.lower()\n",
    "    if ext == \".pdb\": return PDBParser(QUIET=True)\n",
    "    if ext in (\".cif\",\".mmcif\"): return MMCIFParser(QUIET=True)\n",
    "    raise ValueError(f\"Unsupported: {path}\")\n",
    "\n",
    "def load_structure(path: Path):\n",
    "    return _parser_for(path).get_structure(path.stem, str(path))\n",
    "\n",
    "def extract_atom_seq_by_chain(struct) -> Dict[str,str]:\n",
    "    if not BIO_AVAILABLE: return {}\n",
    "    seqs = {}\n",
    "    ppb = PPBuilder()\n",
    "    model = next(iter(struct))\n",
    "    for chain in model:\n",
    "        polypeps = list(ppb.build_peptides(chain, aa_only=False))\n",
    "        if not polypeps: continue\n",
    "        seq = \"\".join([str(pp.get_sequence()) for pp in polypeps])\n",
    "        if not seq: continue\n",
    "        if (MAX_LEN_PER_CHAIN is not None) and (len(seq) > MAX_LEN_PER_CHAIN):\n",
    "            seq = seq[:MAX_LEN_PER_CHAIN]\n",
    "        seqs[chain.id] = seq\n",
    "    return seqs\n",
    "\n",
    "def extract_ca_coords_by_chain(struct) -> Dict[str, List[Tuple[int, np.ndarray]]]:\n",
    "    if not BIO_AVAILABLE: return {}\n",
    "    chain_coords = {}\n",
    "    ppb = PPBuilder()\n",
    "    model = next(iter(struct))\n",
    "    for chain in model:\n",
    "        polypeps = list(ppb.build_peptides(chain))\n",
    "        if not polypeps: continue\n",
    "        coords, off = [], 0\n",
    "        for pp in polypeps:\n",
    "            for i,res in enumerate(pp):\n",
    "                if \"CA\" in res:\n",
    "                    if (MAX_LEN_PER_CHAIN is None) or (off+i < MAX_LEN_PER_CHAIN):\n",
    "                        coords.append((off+i, res[\"CA\"].coord.copy()))\n",
    "            off += len(pp)\n",
    "        if coords: chain_coords[chain.id] = coords\n",
    "    return chain_coords\n",
    "\n",
    "def contact_map_from_coords(coords: List[Tuple[int, np.ndarray]], L: int, thresh: float):\n",
    "    has = np.zeros(L, dtype=bool)\n",
    "    xyz = np.zeros((L,3), dtype=np.float32)\n",
    "    for i,c in coords:\n",
    "        if 0 <= i < L:\n",
    "            has[i] = True; xyz[i] = c\n",
    "    idx = np.where(has)[0]\n",
    "    contact = np.zeros((L,L), dtype=bool)\n",
    "    valid   = np.zeros((L,L), dtype=bool)\n",
    "    if len(idx) > 0:\n",
    "        sub = xyz[idx]\n",
    "        d = np.sqrt(((sub[:,None,:]-sub[None,:,:])**2).sum(-1))\n",
    "        c = (d < thresh)\n",
    "        for a,ia in enumerate(idx):\n",
    "            for b,ib in enumerate(idx):\n",
    "                if ia != ib:\n",
    "                    contact[ia,ib] = c[a,b]\n",
    "                    valid[ia,ib] = True\n",
    "    return contact, valid\n",
    "\n",
    "# ---- Structure → seq/H/contact ----\n",
    "def process_structure(path: Path, tokenizer, esm2_model):\n",
    "    try:\n",
    "        st = load_structure(path)\n",
    "        chain_seqs = extract_atom_seq_by_chain(st)\n",
    "        chain_coords = extract_ca_coords_by_chain(st)\n",
    "        if not chain_seqs: return None\n",
    "\n",
    "        seqs, Hs, Cs, Vs, chain_ids = [], [], [], [], []\n",
    "        for cid in sorted(chain_seqs.keys()):\n",
    "            seq = chain_seqs[cid]\n",
    "            L = len(seq)\n",
    "            C, V = contact_map_from_coords(chain_coords.get(cid, []), L, CA_DIST_THRESH)\n",
    "            if V.sum() == 0:   # no coord pairs\n",
    "                continue\n",
    "            H = embed_sequence(seq, tokenizer, esm2_model)  # returns float32\n",
    "            Hs.append(H); seqs.append(seq); Cs.append(C); Vs.append(V)\n",
    "            chain_ids.extend([cid]*L)\n",
    "\n",
    "        if not Hs: return None\n",
    "\n",
    "        full_seq = \"\".join(seqs)\n",
    "        H_full = torch.cat(Hs, dim=0).to(DEVICE, dtype=torch.float32)\n",
    "        Ls = [len(s) for s in seqs]; Ltot = sum(Ls)\n",
    "        C_full = np.zeros((Ltot,Ltot), dtype=bool)\n",
    "        V_full = np.zeros((Ltot,Ltot), dtype=bool)\n",
    "        ci = 0\n",
    "        for k,L in enumerate(Ls):\n",
    "            cj = ci+L\n",
    "            C_full[ci:cj,ci:cj] = Cs[k]\n",
    "            V_full[ci:cj,ci:cj] = Vs[k]\n",
    "            ci = cj\n",
    "\n",
    "        uniq = {c:i for i,c in enumerate(sorted(set(chain_ids)))}\n",
    "        chain_ids_arr = np.array([uniq[c] for c in chain_ids], dtype=np.int64)\n",
    "        return {\"seq\": full_seq, \"H\": H_full, \"contact\": C_full, \"valid_pair\": V_full,\n",
    "                \"chain_ids\": chain_ids_arr, \"pdb_id\": path.stem, \"path\": str(path)}\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] skip {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---- Balanced sampler + Bilinear scorer ----\n",
    "def build_train_pairs_balanced(S, max_pos=None, neg_ratio=3, upper_tri=True):\n",
    "    C = S[\"contact\"]; V = S[\"valid_pair\"]\n",
    "    if upper_tri:\n",
    "        tri = np.triu(np.ones_like(C, dtype=bool), k=1)\n",
    "    else:\n",
    "        tri = np.ones_like(C, dtype=bool); np.fill_diagonal(tri, False)\n",
    "    pos = C & V & tri\n",
    "    neg = (~C) & V & tri\n",
    "    pos_idx = np.argwhere(pos)\n",
    "    neg_idx = np.argwhere(neg)\n",
    "    if len(pos_idx) == 0:\n",
    "        return np.empty((0,2), dtype=int), np.empty((0,), dtype=np.float32)\n",
    "    if (max_pos is not None) and (len(pos_idx) > max_pos):\n",
    "        pos_idx = pos_idx[np.random.choice(len(pos_idx), size=max_pos, replace=False)]\n",
    "    m = min(len(neg_idx), int(len(pos_idx)*neg_ratio))\n",
    "    if m > 0:\n",
    "        neg_idx = neg_idx[np.random.choice(len(neg_idx), size=m, replace=False)]\n",
    "    else:\n",
    "        neg_idx = np.empty((0,2), dtype=int)\n",
    "    pairs = np.vstack([pos_idx, neg_idx])\n",
    "    labels = np.hstack([np.ones(len(pos_idx), dtype=np.float32),\n",
    "                        np.zeros(len(neg_idx), dtype=np.float32)])\n",
    "    return pairs, labels\n",
    "\n",
    "class BilinearScorer(torch.nn.Module):\n",
    "    def __init__(self, d, rank=128):\n",
    "        super().__init__()\n",
    "        self.U = torch.nn.Linear(d, rank, bias=False)\n",
    "        self.V = torch.nn.Linear(d, rank, bias=False)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(1, dtype=torch.float32))\n",
    "        self.to(torch.float32)\n",
    "    def forward_pairs(self, H, idx):\n",
    "        # Accept numpy or torch indices\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            if idx.numel() == 0:\n",
    "                return torch.empty((0,), device=H.device, dtype=torch.float32)\n",
    "            i = idx[:,0].to(H.device).long()\n",
    "            j = idx[:,1].to(H.device).long()\n",
    "        else:\n",
    "            arr = np.asarray(idx)\n",
    "            if arr.size == 0:\n",
    "                return torch.empty((0,), device=H.device, dtype=torch.float32)\n",
    "            i = torch.as_tensor(arr[:,0], device=H.device, dtype=torch.long)\n",
    "            j = torch.as_tensor(arr[:,1], device=H.device, dtype=torch.long)\n",
    "        H = H.to(dtype=torch.float32)  # ensure fp32 for Linear\n",
    "        A = self.U(H); B = self.V(H)\n",
    "        s = (A[i]*B[j]).sum(-1) + self.bias\n",
    "        return s.to(dtype=torch.float32).view(-1)  # ensure (N,) fp32\n",
    "\n",
    "# ---- Training helpers ----\n",
    "def iterate_balanced_pairs(structs, max_pos=None, neg_ratio=3.0, batch_pairs=50000, shuffle=True):\n",
    "    \"\"\"Yields (H, pairs, labels) with pairs/labels already on DEVICE.\"\"\"\n",
    "    for S in structs:\n",
    "        if S is None:\n",
    "            continue\n",
    "        pairs, labels = build_train_pairs_balanced(S, max_pos=max_pos, neg_ratio=neg_ratio)\n",
    "        if len(pairs) == 0:\n",
    "            continue\n",
    "        H = S[\"H\"]  # will be moved/casted in the train loop\n",
    "        pairs_tensor = torch.from_numpy(pairs).to(DEVICE)              # long later\n",
    "        labels_tensor = torch.from_numpy(labels).to(DEVICE)            # float later\n",
    "        if shuffle:\n",
    "            perm = torch.randperm(len(pairs_tensor), device=pairs_tensor.device)\n",
    "            pairs_tensor = pairs_tensor[perm]\n",
    "            labels_tensor = labels_tensor[perm]\n",
    "        for i in range(0, len(pairs_tensor), batch_pairs):\n",
    "            batch_pairs_tensor = pairs_tensor[i:i+batch_pairs]\n",
    "            batch_labels_tensor = labels_tensor[i:i+batch_pairs]\n",
    "            yield H, batch_pairs_tensor, batch_labels_tensor\n",
    "\n",
    "def _align_logits_targets_f32(logits: torch.Tensor, y: torch.Tensor):\n",
    "    dev = logits.device\n",
    "    logits = logits.to(device=dev, dtype=torch.float32)\n",
    "    y = y.to(device=dev, dtype=torch.float32)\n",
    "    if (torch.min(y) < 0) or (torch.max(y) > 1):\n",
    "        y = (y > 0).to(torch.float32)\n",
    "    if logits.ndim == 2 and logits.size(1) == 1 and y.ndim == 1:\n",
    "        y = y.unsqueeze(1)\n",
    "    if logits.shape != y.shape:\n",
    "        try:\n",
    "            y = y.view_as(logits)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"[ALIGN] Shape mismatch: logits {logits.shape}, target {y.shape}\") from e\n",
    "    return logits, y\n",
    "\n",
    "def train_one_epoch_balanced(\n",
    "    model, opt, train_structs,\n",
    "    max_pos=None, neg_ratio: float = 3.0, batch_pairs: int = 50000,\n",
    "    pos_weight=None, amp: bool = False, debug_first_batch: bool = False,\n",
    "):\n",
    "    \"\"\"BCEWithLogitsLoss-safe loop: float32 everywhere + aligned shapes/devices.\"\"\"\n",
    "    model.train(); total_loss = 0.0\n",
    "    # BCE loss\n",
    "    if pos_weight is not None:\n",
    "        pos_weight_tensor = torch.tensor([pos_weight], device=DEVICE, dtype=torch.float32)\n",
    "        bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "    else:\n",
    "        bce = nn.BCEWithLogitsLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(amp and DEVICE.type==\"cuda\"))\n",
    "    first = True\n",
    "    for H, pairs, labels in iterate_balanced_pairs(train_structs, max_pos=max_pos, neg_ratio=neg_ratio, batch_pairs=batch_pairs):\n",
    "        # Move/cast inputs\n",
    "        H = H.to(device=DEVICE, dtype=torch.float32, non_blocking=True)\n",
    "        pairs = pairs.to(device=DEVICE, dtype=torch.long,    non_blocking=True)\n",
    "        labels = labels.to(device=DEVICE, dtype=torch.float32, non_blocking=True).view(-1,1)\n",
    "        # Forward + loss\n",
    "        if amp and DEVICE.type==\"cuda\":\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model.forward_pairs(H, pairs)\n",
    "                logits, labels = _align_logits_targets_f32(logits, labels)\n",
    "                loss = bce(logits, labels)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
    "        else:\n",
    "            logits = model.forward_pairs(H, pairs)\n",
    "            logits, labels = _align_logits_targets_f32(logits, labels)\n",
    "            loss = bce(logits, labels)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "        if debug_first_batch and first:\n",
    "            print(f\"[DEBUG] H:{H.dtype}/{H.device}/{tuple(H.shape)}  pairs:{pairs.dtype}/{pairs.device}/{tuple(pairs.shape)}\")\n",
    "            print(f\"[DEBUG] logits:{logits.dtype}/{logits.device}/{tuple(logits.shape)}  labels:{labels.dtype}/{labels.device}/{tuple(labels.shape)}\")\n",
    "            first = False\n",
    "        total_loss += float(loss.detach().cpu())\n",
    "    return total_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_structs_streamed(model, structs, chunk=200000):\n",
    "    model.eval()\n",
    "    per_pdb = {}; all_p=[]; all_y=[]\n",
    "    for S in structs:\n",
    "        if S is None: continue\n",
    "        H = S[\"H\"].to(DEVICE, dtype=torch.float32)\n",
    "        C = S[\"contact\"]; V = S[\"valid_pair\"]\n",
    "        tri = np.triu(np.ones_like(C, dtype=bool), k=1)\n",
    "        mask = V & tri\n",
    "        idx = np.argwhere(mask)\n",
    "        if idx.size == 0: continue\n",
    "        probs_list=[]; labels_list=[]\n",
    "        for st in range(0, len(idx), chunk):\n",
    "            ib = idx[st:st+chunk]\n",
    "            logits = model.forward_pairs(H, ib)  # accepts numpy indices; returns fp32\n",
    "            probs  = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            labs   = C[ib[:,0], ib[:,1]].astype(int)\n",
    "            probs_list.append(probs); labels_list.append(labs)\n",
    "        probs = np.concatenate(probs_list); labels = np.concatenate(labels_list)\n",
    "        try:\n",
    "            ap = average_precision_score(labels, probs)\n",
    "            roc= roc_auc_score(labels, probs)\n",
    "        except Exception:\n",
    "            ap, roc = float(\"nan\"), float(\"nan\")\n",
    "        L = len(S[\"seq\"]); order = probs.argsort()[::-1]\n",
    "        def p_at(k):\n",
    "            k = max(1, min(len(order), k)); sel = order[:k]\n",
    "            return labels[sel].mean()\n",
    "        per_pdb[S.get(\"pdb_id\",\"?\")] = dict(pr_auc=ap, roc_auc=roc, p_at_L=p_at(L), p_at_L2=p_at(max(1,L//2)), p_at_L5=p_at(max(1,L//5)))\n",
    "        all_p.append(probs); all_y.append(labels)\n",
    "        del probs_list, labels_list, probs, labels; gc.collect()\n",
    "    if not all_p:\n",
    "        return dict(global_pr_auc=float(\"nan\"), global_roc_auc=float(\"nan\"), per_pdb=per_pdb)\n",
    "    P = np.concatenate(all_p); Y = np.concatenate(all_y)\n",
    "    try:\n",
    "        g_ap = average_precision_score(Y, P); g_roc = roc_auc_score(Y, P)\n",
    "    except Exception:\n",
    "        g_ap, g_roc = float(\"nan\"), float(\"nan\")\n",
    "    return dict(global_pr_auc=g_ap, global_roc_auc=g_roc, per_pdb=per_pdb)\n",
    "\n",
    "# ---- Dummy data ----\n",
    "def create_dummy_structure(pdb_id, seq_length=100, d_model=320):\n",
    "    seq = \"A\" * seq_length\n",
    "    H = torch.randn(seq_length, d_model, device=DEVICE, dtype=torch.float32)\n",
    "    contact = np.random.rand(seq_length, seq_length) < 0.1\n",
    "    np.fill_diagonal(contact, False)\n",
    "    valid_pair = np.triu(np.ones((seq_length, seq_length), dtype=bool), k=1)\n",
    "    return {\"seq\": seq, \"H\": H, \"contact\": contact, \"valid_pair\": valid_pair,\n",
    "            \"chain_ids\": np.zeros(seq_length, dtype=np.int64), \"pdb_id\": pdb_id, \"path\": f\"dummy_{pdb_id}.pdb\"}\n",
    "\n",
    "# ---- Main ----\n",
    "def main():\n",
    "    def collect_files(folder: Path):\n",
    "        pats = (\"*.pdb\",\"*.PDB\",\"*.cif\",\"*.CIF\",\"*.mmcif\",\"*.MMCIF\")\n",
    "        files=[]; [files.extend(folder.glob(p)) for p in pats]\n",
    "        return sorted(files)\n",
    "\n",
    "    print(\"Loading ESM2 model...\")\n",
    "    tokenizer, esm2_model = try_load_esm2(MODEL_ID)\n",
    "    print(\"Embedder:\", type(esm2_model).__name__)\n",
    "\n",
    "    train_all = collect_files(PDB_TRAIN_DIR)\n",
    "    test_files = collect_files(PDB_TEST_DIR)\n",
    "    print(f\"Found {len(train_all)} train, {len(test_files)} test files.\")\n",
    "\n",
    "    if len(train_all) == 0:\n",
    "        print(\"WARNING: No training files found! Using dummy data for demonstration.\")\n",
    "        train_structs = [create_dummy_structure(f\"train_{i}\") for i in range(5)]\n",
    "        val_structs   = [create_dummy_structure(f\"val_{i}\") for i in range(2)]\n",
    "        test_structs  = [create_dummy_structure(f\"test_{i}\") for i in range(2)]\n",
    "    else:\n",
    "        if (MAX_TRAIN_FILES is not None) and (len(train_all) > MAX_TRAIN_FILES):\n",
    "            train_all = train_all[:MAX_TRAIN_FILES]; print(f\"Capped train files to {len(train_all)}\")\n",
    "        train_files, val_files = train_test_split(train_all, test_size=VAL_SPLIT, random_state=SEED)\n",
    "        print(f\"Split: {len(train_files)} train / {len(val_files)} val\")\n",
    "        print(\"Processing train/val (caches embeddings per chain on first run)...\")\n",
    "        train_structs = [process_structure(p, tokenizer, esm2_model) for p in tqdm(train_files)]\n",
    "        val_structs   = [process_structure(p, tokenizer, esm2_model) for p in tqdm(val_files)]\n",
    "        train_structs = [s for s in train_structs if s is not None]\n",
    "        val_structs   = [s for s in val_structs if s is not None]\n",
    "        if len(train_structs) == 0:\n",
    "            print(\"WARNING: No training structures found! Creating dummy data.\")\n",
    "            train_structs = [create_dummy_structure(f\"train_{i}\") for i in range(5)]\n",
    "            val_structs   = [create_dummy_structure(f\"val_{i}\") for i in range(2)]\n",
    "\n",
    "    d = int(train_structs[0][\"H\"].shape[-1]); print(f\"Embedding dimension: {d}\")\n",
    "    model = BilinearScorer(d=d, rank=RANK).to(DEVICE).to(torch.float32)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for ep in range(1, EPOCHS + 1):\n",
    "        loss = train_one_epoch_balanced(model, opt, train_structs, debug_first_batch=(ep == 1))\n",
    "        val_scores = evaluate_structs_streamed(model, val_structs)\n",
    "        print(f\"[epoch {ep}] loss={loss:.3f}  val PR-AUC={val_scores['global_pr_auc']:.4f}  ROC-AUC={val_scores['global_roc_auc']:.4f}\")\n",
    "\n",
    "    torch.save({\"state_dict\": model.state_dict(), \"cfg\": {\"d\": d, \"rank\": RANK, \"lr\": LR, \"epochs\": EPOCHS}}, str(MODEL_PATH))\n",
    "    print(\"Saved:\", MODEL_PATH)\n",
    "\n",
    "    print(\"Processing test...\")\n",
    "    if len(test_files) > 0:\n",
    "        test_structs = [process_structure(p, tokenizer, esm2_model) for p in tqdm(test_files)]\n",
    "        test_structs = [s for s in test_structs if s is not None]\n",
    "    else:\n",
    "        test_structs = [create_dummy_structure(f\"test_{i}\") for i in range(2)]\n",
    "    scores = evaluate_structs_streamed(model, test_structs)\n",
    "    print(\"TEST  PR-AUC={:.4f}  ROC-AUC={:.4f}\".format(scores[\"global_pr_auc\"], scores[\"global_roc_auc\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
