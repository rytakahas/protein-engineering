{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 137176,
     "status": "ok",
     "timestamp": 1742650645683,
     "user": {
      "displayName": "Ryoji Takahashi",
      "userId": "08099237406056068712"
     },
     "user_tz": -60
    },
    "id": "7TfzYEa2qLz4",
    "outputId": "c0fb4012-1f22-4dff-a19b-3403d6192704"
   },
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install torch transformers scikit-learn pandas numpy biopython peft bitsandbytes requests optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23871,
     "status": "ok",
     "timestamp": 1742650669570,
     "user": {
      "displayName": "Ryoji Takahashi",
      "userId": "08099237406056068712"
     },
     "user_tz": -60
    },
    "id": "oCl5Gk3GUaM_",
    "outputId": "ce97b5e3-f603-4c2b-a716-6b306a6c4141"
   },
   "outputs": [],
   "source": [
    "# Install necessary tools in Google Colab\n",
    "!apt-get install -y mafft\n",
    "!apt-get install -y hmmer\n",
    "!wget https://github.com/soedinglab/MMseqs2/releases/download/17-b804f/mmseqs-linux-gpu.tar.gz\n",
    "!tar xvf mmseqs-linux-gpu.tar.gz\n",
    "!chmod +x mmseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from Bio import AlignIO\n",
    "from Bio import SeqIO\n",
    "import optuna\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, subprocess\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from Bio import AlignIO\n",
    "\n",
    "WT_SEQUENCE = \"MRHGDISSSNDTVGVAVVNYKMPRLHTAAEVLDNARKIAEMIVGMKQGLPGMDLVVFPEYSLQGIMYDPAEMMETAVAIPGEETEIFSRACRKANVWGVFSLTGERHEEHPRKAPYNTLVLIDNNGEIVQKYRKIIPWCPIEGWYPGGQTYVSEGPKGMKISLIICDDGNYPEIWRDCAMKGAELIVRCQGYMYPAKDQQVMMAKAMAWANNCYVAVANAAGFDGVYSYFGHSAIIGFDGRTLGECGEEEMGIQYAQLSLSQIRDARANDQSQNHLFKILHRGYSGLQASGDGDRGLAECPFEFYRTWVTDAEKARENVERLTRSTTGVAQCPVGRLPYEG\"\n",
    "BLAST_URL = \"https://blast.ncbi.nlm.nih.gov/Blast.cgi\"\n",
    "\n",
    "# ======= Step 1: BLAST API for Homologs =======\n",
    "def run_blast_search(wt_sequence, identity_threshold=90.0, max_retries=30, sleep_time=10, min_length=100, hitlist_size=300):\n",
    "    \"\"\"Run BLAST and extract homologous sequences below a given identity threshold.\"\"\"\n",
    "    params = {\n",
    "        \"CMD\": \"Put\",\n",
    "        \"PROGRAM\": \"blastp\",\n",
    "        \"DATABASE\": \"nr\",\n",
    "        \"QUERY\": wt_sequence,\n",
    "        \"FORMAT_TYPE\": \"XML\",\n",
    "        \"EXPECT\": \"1e-2\",\n",
    "        \"HITLIST_SIZE\": str(hitlist_size)\n",
    "    }\n",
    "    response = requests.post(BLAST_URL, data=params)\n",
    "    response.raise_for_status()\n",
    "    response_text = response.text\n",
    "    if \"RID = \" not in response_text:\n",
    "        raise Exception(\"No RID found in BLAST response.\")\n",
    "    rid = response_text.split(\"RID = \")[-1].split(\"\\n\")[0].strip()\n",
    "    print(f\"BLAST RID: {rid}\")\n",
    "\n",
    "    # Wait for completion\n",
    "    for attempt in range(max_retries):\n",
    "        status = requests.get(BLAST_URL, params={\"CMD\":\"Get\", \"FORMAT_OBJECT\":\"SearchInfo\", \"RID\":rid})\n",
    "        if \"Status=READY\" in status.text:\n",
    "            print(\"BLAST complete.\")\n",
    "            break\n",
    "        print(f\"Waiting... {attempt+1}/{max_retries}\")\n",
    "        time.sleep(sleep_time)\n",
    "    else:\n",
    "        raise Exception(\"BLAST timed out\")\n",
    "\n",
    "    # Download results\n",
    "    result = requests.get(BLAST_URL, params={\"CMD\":\"Get\", \"FORMAT_TYPE\":\"XML\", \"RID\":rid})\n",
    "    result.raise_for_status()\n",
    "    root = ET.fromstring(result.text)\n",
    "    seqs = []\n",
    "    for hit in root.findall(\".//Hit\"):\n",
    "        for hsp in hit.findall(\".//Hsp\"):\n",
    "            hseq_elem = hsp.find(\"Hsp_hseq\")\n",
    "            identity_elem = hsp.find(\"Hsp_identity\")\n",
    "            align_len_elem = hsp.find(\"Hsp_align-len\")\n",
    "            if hseq_elem is not None and identity_elem is not None and align_len_elem is not None:\n",
    "                hseq = hseq_elem.text.strip()\n",
    "                identity = int(identity_elem.text)\n",
    "                align_len = int(align_len_elem.text)\n",
    "                identity_pct = 100 * identity / align_len\n",
    "                if identity_pct < identity_threshold and len(hseq) > min_length:\n",
    "                    seqs.append(hseq)\n",
    "    seqs = [wt_sequence] + list({s for s in seqs if s != wt_sequence})  # unique, include WT\n",
    "    print(f\"Total homologs: {len(seqs)}\")\n",
    "    # Save to FASTA\n",
    "    with open(\"msa_input.fasta\", \"w\") as f:\n",
    "        for i, s in enumerate(seqs):\n",
    "            f.write(f\">seq{i}\\n{s}\\n\")\n",
    "    return \"msa_input.fasta\"\n",
    "\n",
    "# ======= Step 2: Align with MAFFT =======\n",
    "def run_mafft(input_fasta, output_fasta=\"msa_aligned.fasta\"):\n",
    "    print(f\"Running MAFFT alignment...\")\n",
    "    cmd = f\"mafft --auto {input_fasta} > {output_fasta}\"\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "    print(f\"Alignment written: {output_fasta}\")\n",
    "    return output_fasta\n",
    "\n",
    "# ======= Step 3: Calculate Henikoff Weights =======\n",
    "def henikoff_weights(msa_file, format=\"fasta\"):\n",
    "    alignment = AlignIO.read(msa_file, format)\n",
    "    n_seq = len(alignment)\n",
    "    aln_len = alignment.get_alignment_length()\n",
    "    weights = np.zeros(n_seq)\n",
    "    for pos in range(aln_len):\n",
    "        aa_counts = {}\n",
    "        for record in alignment:\n",
    "            aa = record.seq[pos]\n",
    "            if aa not in aa_counts:\n",
    "                aa_counts[aa] = 0\n",
    "            aa_counts[aa] += 1\n",
    "        n_types = len(aa_counts)\n",
    "        for i, record in enumerate(alignment):\n",
    "            aa = record.seq[pos]\n",
    "            weights[i] += 1.0 / (n_types * aa_counts[aa])\n",
    "    weights /= weights.sum()\n",
    "    return weights\n",
    "\n",
    "# ======= (Optional) Jackhmmer/MMseqs2 integration (not changed here) =======\n",
    "\n",
    "# ==== MAIN WORKFLOW ====\n",
    "method = \"blast\"  # \"jackhmmer\" or \"mmseqs2\" possible if implemented\n",
    "\n",
    "if method == \"blast\":\n",
    "    msa_input = run_blast_search(WT_SEQUENCE, identity_threshold=90.0, hitlist_size=500)\n",
    "elif method == \"jackhmmer\":\n",
    "    msa_input = run_jackhmmer_search(WT_SEQUENCE)\n",
    "elif method == \"mmseqs2\":\n",
    "    msa_input = run_mmseqs2_search(WT_SEQUENCE)\n",
    "else:\n",
    "    raise ValueError(\"Invalid method chosen. Please select 'blast', 'jackhmmer', or 'mmseqs2'.\")\n",
    "\n",
    "msa_aligned = run_mafft(msa_input)\n",
    "\n",
    "weights = henikoff_weights(msa_aligned, \"fasta\")\n",
    "print(\"Sequence weights:\", weights)\n",
    "np.save(\"msa_weights.npy\", weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0c612b3640044c7ebd71addcde9ced72",
      "da307c320af342e98b825a96a197a7dc",
      "7b19f04e6bfe410e8a0a317a251bb9a8",
      "71747f0b96ec4a27ab16e61d12b53619",
      "b4932c25503d44e7b230ba212261c230",
      "12b756dcf8fb4a7eb7b44866348ec27e",
      "b3fd28b80fa04b0c9a0ac11321c150ea",
      "c313acb95ed24aeb90f0b9b501b2eb0e",
      "21b82ac01198443aa47bf252d37b60c6",
      "b63daad3c9024ec3941a1a444670c070",
      "50a31ad977ba4e308d185d2fa89de112",
      "50ab94e403b64f92bd2ce70fe623caad",
      "778123140f134a87b8f39d84dd22b2a3",
      "7ea73ba579c54212bed88775b0025dc6",
      "af35b6f69ffd4a1282f749832264341a",
      "b79c5bce60a14310aed52c34bdcac665",
      "e2436ade453040f38092b506d3d643bc",
      "ec51b687539e4819919d47d5de287816",
      "d918413ff9a44e59be58098e628caeec",
      "f0a4e5bd40904edaad31223a256312dc",
      "a8461c0929c64aeb9aa9917875e1bd50",
      "0dc20ad7eff14ef980d5f7d2acb308fc",
      "b573f271ac844206a908456f14ffa656",
      "cf5d1f668166418bb7d04a3bd983c380",
      "34139b6d953941c296e00909c6550585",
      "4637a39c72e14cacad3c748479005287",
      "a9a98c30e1914ee58c11cb6a7f43a461",
      "b5aba31c7ae242fe887f3ebc209ed61c",
      "d9b1e7526298404a9aeb3ff2f43edd7e",
      "d5d36048c2a948bb9d0a6f1a66abe21d",
      "bb20afce1030419985b5b559b52fce90",
      "f7f40c96d28a489e9de26c5ac4b9fd0c",
      "f5236a66cca54eb296a59c22c9059598",
      "db40349d8c254cf2b0774f779c1cf7e9",
      "9cb65ef731d34f3d92c125e1cb81d8f7",
      "02b9125c916d46edaec385c770f50b7a",
      "fd03f82c8caa4ac398edaca0c437fe50",
      "b9cd6283056144578306cf0fda4c8ff8",
      "123b1d2f682d489995598aa6004a76cf",
      "bed93adcb3cd46238730ff1ae5b94d5d",
      "5988eedb571545c3844773966bc262fe",
      "1912299bda314408b07fe15a22a02b84",
      "c8cd6a8c41244adaa96548de5fa32cbd",
      "fcd0692ed55b4d8fbf2ed181e1193f5c",
      "3944573a47654ec69fb589c7fd1f11ca",
      "30028e1c602d4934a44ebb654bee2196",
      "69e8801ec44645648cf35025f43b719d",
      "e9950188d0c44977954dd185bc718517",
      "15a7f2f1f09b4adf9c49531c5fcd9eeb",
      "a057799156de426a9d30bd977505eb02",
      "6c93bd29dd3d4a9aa63cebe525ca1d37",
      "f77e5dbd8dfb4e389dec8654572fbcb6",
      "81892f95210f48559de697db590e044e",
      "b127d89066e64c048596f293b672687c",
      "d24e651a6f874a388a99e4d89ab0c56f",
      "eb05579a285a45cb9e2b5978092c71ba",
      "e60cb77ca6db4bee81cc94c340f3fef2",
      "fc7ebb24306c4488ac3b90a7dbb249b4",
      "6a70b6fc1acd4a15a56f0af196f2cbb4",
      "d633351cd08147a1b16bca7a87d09d5d",
      "56fd2cd9d39b4d94ac77faa10939c932",
      "94ab8f73ba324a7abcd14f51796633fa",
      "d00de2a066d846099af65e88966e46e3",
      "989a05586f4e4bd19494ef9fe5e48ba0",
      "7dba158b31664804a6335de65cb89730",
      "e591032694314fab9701bd8359d0ac91"
     ]
    },
    "executionInfo": {
     "elapsed": 612416,
     "status": "ok",
     "timestamp": 1742651281990,
     "user": {
      "displayName": "Ryoji Takahashi",
      "userId": "08099237406056068712"
     },
     "user_tz": -60
    },
    "id": "xGLKt38Kta0W",
    "outputId": "3fc9e563-e0bd-4212-e522-5e6545676897"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "\n",
    "# --- DEVICE ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load ProtBERT ---\n",
    "print(\"Loading ProtBERT...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\")\n",
    "base_model = AutoModelForMaskedLM.from_pretrained(\"Rostlab/prot_bert\").to(device)\n",
    "\n",
    "# --- Embedding extractor ---\n",
    "def get_protbert_embedding(model, sequence):\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer(sequence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        outputs = model(**tokens, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        embedding = last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "        return embedding\n",
    "\n",
    "# --- Mutation utility ---\n",
    "def generate_mutated_sequence(wt_sequence, mutation_string):\n",
    "    seq_list = list(wt_sequence)\n",
    "    mutations = mutation_string.split(',')\n",
    "    for mut in mutations:\n",
    "        if len(mut) >= 3 and mut[1:-1].isdigit():\n",
    "            pos = int(mut[1:-1]) - 1\n",
    "            if pos < len(seq_list) and mut[-1] != '*':\n",
    "                seq_list[pos] = mut[-1]\n",
    "    return ''.join(seq_list)\n",
    "\n",
    "# --- Load data ---\n",
    "print(\"Loading dataset...\")\n",
    "url = \"https://figshare.com/ndownloader/files/7337543\"\n",
    "df = pd.read_csv(url, sep='\\t')\n",
    "df.rename(columns={'mutation': 'mutation_string', 'normalized_fitness': 'fitness'}, inplace=True)\n",
    "df['fitness'] = pd.to_numeric(df['fitness'], errors='coerce')\n",
    "df.dropna(subset=['fitness'], inplace=True)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)   # SHUFFLE\n",
    "# Outlier handling (optional, clip to [-5, 5] or domain-specific)\n",
    "df['fitness'] = df['fitness'].clip(-5, 5)\n",
    "\n",
    "WT_SEQUENCE = \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLTYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n",
    "df['mutated_sequence'] = df['mutation_string'].apply(lambda x: generate_mutated_sequence(WT_SEQUENCE, x))\n",
    "\n",
    "# --- Generate embeddings ---\n",
    "print(\"Generating embeddings... (may take a while)\")\n",
    "embeddings, y = [], []\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        emb = get_protbert_embedding(base_model, row['mutated_sequence'])\n",
    "        embeddings.append(emb)\n",
    "        y.append(row['fitness'])\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping sequence due to error: {e}\")\n",
    "X = np.vstack(embeddings)\n",
    "y = np.array(y)\n",
    "\n",
    "# --- Scale features ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# --- Train/test split (shuffle again just in case) ---\n",
    "all_indices = np.arange(len(X_scaled))\n",
    "X_train_index, X_test_index = train_test_split(all_indices, test_size=0.2, random_state=42)\n",
    "X_train, X_test = X_scaled[X_train_index], X_scaled[X_test_index]\n",
    "y_train, y_test = y[X_train_index], y[X_test_index]\n",
    "\n",
    "print(f\"Train targets variance: {np.var(y_train):.3f}, Test targets variance: {np.var(y_test):.3f}\")\n",
    "\n",
    "# ---- Sample weights (optional, all ones) ----\n",
    "weights = np.ones(len(X_scaled))\n",
    "train_weights = weights[X_train_index]\n",
    "\n",
    "# --- XGBoost + Optuna ---\n",
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 16),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 0.6),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 0.6)\n",
    "    }\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(X_train[train_idx], y_train[train_idx], sample_weight=train_weights[train_idx])\n",
    "        preds = model.predict(X_train[val_idx])\n",
    "        mse_scores.append(mean_squared_error(y_train[val_idx], preds))\n",
    "    return np.mean(mse_scores)\n",
    "\n",
    "print(\"Running Optuna for XGBoost...\")\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(xgb_objective, n_trials=15)\n",
    "xgb_params = study.best_params\n",
    "xgb_params['tree_method'] = 'hist'\n",
    "xgb_params['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Best XGBoost params:\", xgb_params)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(X_train, y_train, sample_weight=train_weights)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Diagnostics: y_test and y_pred_xgb ---\")\n",
    "print(\"y_test: min\", np.min(y_test), \"max\", np.max(y_test), \"unique\", np.unique(y_test)[:10], \"std\", np.std(y_test))\n",
    "print(\"y_pred_xgb: min\", np.min(y_pred_xgb), \"max\", np.max(y_pred_xgb), \"unique\", np.unique(np.round(y_pred_xgb,3))[:10], \"std\", np.std(y_pred_xgb))\n",
    "\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "if np.std(y_pred_xgb) < 1e-8 or np.std(y_test) < 1e-8:\n",
    "    pearson_xgb = 0.0\n",
    "else:\n",
    "    pearson_xgb, _ = pearsonr(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Test MSE: {mse_xgb:.4f}  Pearson: {pearson_xgb:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred_xgb, color='green', alpha=0.6)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
    "plt.xlabel('Actual Fitness')\n",
    "plt.ylabel('Predicted Fitness')\n",
    "plt.title('Actual vs Predicted Fitness (XGBoost)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ---- Cross-validation: report mean Pearson/MSE on all data ----\n",
    "print(\"\\n--- Cross-validation on all data (XGBoost) ---\")\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "pearson_scores = []\n",
    "mse_scores = []\n",
    "for train_idx, test_idx in kf.split(X_scaled):\n",
    "    model = xgb.XGBRegressor(**xgb_params)\n",
    "    model.fit(X_scaled[train_idx], y[train_idx], sample_weight=weights[train_idx])\n",
    "    y_pred = model.predict(X_scaled[test_idx])\n",
    "    mse_scores.append(mean_squared_error(y[test_idx], y_pred))\n",
    "    # Pearson\n",
    "    if np.std(y_pred) < 1e-8 or np.std(y[test_idx]) < 1e-8:\n",
    "        p = 0.0\n",
    "    else:\n",
    "        p, _ = pearsonr(y[test_idx], y_pred)\n",
    "    pearson_scores.append(p)\n",
    "print(f\"Mean CV MSE: {np.mean(mse_scores):.4f}, Mean CV Pearson: {np.mean(pearson_scores):.4f}\")\n",
    "\n",
    "# ---- Linear Models: Lasso, Ridge, ElasticNet ----\n",
    "def train_and_evaluate_model(model, param_grid, X_train, y_train, X_test, y_test, train_weights, model_name):\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train, y_train, cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        fit_params={'sample_weight': train_weights}\n",
    "    )\n",
    "    print(f\"{model_name} - CV MSE: {-cv_scores.mean():.4f}\")\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train, sample_weight=train_weights)\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    if np.std(y_pred) < 1e-8 or np.std(y_test) < 1e-8:\n",
    "        pearson_corr = 0.0\n",
    "    else:\n",
    "        pearson_corr, _ = pearsonr(y_test, y_pred)\n",
    "    print(f\"MSE - {model_name}: {mse:.4f}\")\n",
    "    print(f\"Pearson Corr - {model_name}: {pearson_corr:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_pred, color='blue', alpha=0.6)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
    "    plt.xlabel('Actual Fitness')\n",
    "    plt.ylabel('Predicted Fitness')\n",
    "    plt.title(f'Actual vs Predicted Fitness ({model_name})')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return grid_search\n",
    "\n",
    "param_grid_ridge = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "param_grid_lasso = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "param_grid_en = {'alpha': [0.01, 0.1, 1, 10, 100], 'l1_ratio': [0.1, 0.5, 0.9, 1.0]}\n",
    "\n",
    "ridge_model = Ridge()\n",
    "lasso_model = Lasso()\n",
    "elastic_net_model = ElasticNet()\n",
    "\n",
    "train_and_evaluate_model(ridge_model, param_grid_ridge, X_train, y_train, X_test, y_test, train_weights, \"Ridge\")\n",
    "train_and_evaluate_model(lasso_model, param_grid_lasso, X_train, y_train, X_test, y_test, train_weights, \"Lasso\")\n",
    "train_and_evaluate_model(elastic_net_model, param_grid_en, X_train, y_train, X_test, y_test, train_weights, \"ElasticNet\")\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import os.path\n",
    "os.chdir(\"set a path here\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import transformers, datasets\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.t5.modeling_t5 import T5Config, T5PreTrainedModel, T5Stack\n",
    "from transformers.utils.model_parallel_utils import assert_device_map, get_device_map\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from transformers import TrainingArguments, Trainer, set_seed\n",
    "\n",
    "from evaluate import load\n",
    "from datasets import Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt # Set environment variables to run Deepspeed from a notebook\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\" # For this example we import the \"three_vs_rest\" GB1 dataset from https://github.com/J-SNACKKB/FLIP\n",
    "# For details, see publication here: https://openreview.net/forum?id=p2dMLEwL8tF\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "# Download the zip file from GitHub\n",
    "url = 'https://github.com/J-SNACKKB/FLIP/raw/main/splits/gb1/splits.zip'\n",
    "response = requests.get(url)\n",
    "zip_file = zipfile.ZipFile(BytesIO(response.content))\n",
    "\n",
    "# Load the `three_vs_rest.csv` file into a pandas dataframe\n",
    "with zip_file.open('splits/three_vs_rest.csv') as file:\n",
    "    df = pd.read_csv(file) # Drop test data\n",
    "df=df[df.set==\"train\"]\n",
    "\n",
    "# Get train and validation data\n",
    "my_train=df[df.validation!=True].reset_index(drop=True)\n",
    "my_valid=df[df.validation==True].reset_index(drop=True)\n",
    "\n",
    "# Set column names to \"sequence\" and \"label\"\n",
    "my_train.columns=[\"sequence\",\"label\"]+list(my_train.columns[2:])\n",
    "my_valid.columns=[\"sequence\",\"label\"]+list(my_valid.columns[2:])\n",
    "\n",
    "# Drop unneeded columns\n",
    "my_train=my_train[[\"sequence\",\"label\"]]\n",
    "my_valid=my_valid[[\"sequence\",\"label\"]] # Modifies an existing transformer and introduce the LoRA layers\n",
    "\n",
    "class LoRAConfig:\n",
    "    def __init__(self):\n",
    "        self.lora_rank = 4\n",
    "        self.lora_init_scale = 0.01\n",
    "        self.lora_modules = \".*SelfAttention|.*EncDecAttention\"\n",
    "        self.lora_layers = \"q|k|v|o\"\n",
    "        self.trainable_param_names = \".*layer_norm.*|.*lora_[ab].*\"\n",
    "        self.lora_scaling_rank = 1\n",
    "        # lora_modules and lora_layers are speicified with regular expressions\n",
    "        # see https://www.w3schools.com/python/python_regex.asp for reference\n",
    "        \n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer, rank, scaling_rank, init_scale):\n",
    "        super().__init__()\n",
    "        self.in_features = linear_layer.in_features\n",
    "        self.out_features = linear_layer.out_features\n",
    "        self.rank = rank\n",
    "        self.scaling_rank = scaling_rank\n",
    "        self.weight = linear_layer.weight\n",
    "        self.bias = linear_layer.bias\n",
    "        if self.rank > 0:\n",
    "            self.lora_a = nn.Parameter(torch.randn(rank, linear_layer.in_features) * init_scale)\n",
    "            if init_scale < 0:\n",
    "                self.lora_b = nn.Parameter(torch.randn(linear_layer.out_features, rank) * init_scale)\n",
    "            else:\n",
    "                self.lora_b = nn.Parameter(torch.zeros(linear_layer.out_features, rank))\n",
    "        if self.scaling_rank:\n",
    "            self.multi_lora_a = nn.Parameter(\n",
    "                torch.ones(self.scaling_rank, linear_layer.in_features)\n",
    "                + torch.randn(self.scaling_rank, linear_layer.in_features) * init_scale\n",
    "            )\n",
    "            if init_scale < 0:\n",
    "                self.multi_lora_b = nn.Parameter(\n",
    "                    torch.ones(linear_layer.out_features, self.scaling_rank)\n",
    "                    + torch.randn(linear_layer.out_features, self.scaling_rank) * init_scale\n",
    "                )\n",
    "            else:\n",
    "                self.multi_lora_b = nn.Parameter(torch.ones(linear_layer.out_features, self.scaling_rank))\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.scaling_rank == 1 and self.rank == 0:\n",
    "            # parsimonious implementation for ia3 and lora scaling\n",
    "            if self.multi_lora_a.requires_grad:\n",
    "                hidden = F.linear((input * self.multi_lora_a.flatten()), self.weight, self.bias)\n",
    "            else:\n",
    "                hidden = F.linear(input, self.weight, self.bias)\n",
    "            if self.multi_lora_b.requires_grad:\n",
    "                hidden = hidden * self.multi_lora_b.flatten()\n",
    "            return hidden\n",
    "        else:\n",
    "            # general implementation for lora (adding and scaling)\n",
    "            weight = self.weight\n",
    "            if self.scaling_rank:\n",
    "                weight = weight * torch.matmul(self.multi_lora_b, self.multi_lora_a) / self.scaling_rank\n",
    "            if self.rank:\n",
    "                weight = weight + torch.matmul(self.lora_b, self.lora_a) / self.rank\n",
    "            return F.linear(input, weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"in_features={}, out_features={}, bias={}, rank={}, scaling_rank={}\".format(\n",
    "            self.in_features, self.out_features, self.bias is not None, self.rank, self.scaling_rank\n",
    "        )\n",
    "\n",
    "\n",
    "def modify_with_lora(transformer, config):\n",
    "    for m_name, module in dict(transformer.named_modules()).items():\n",
    "        if re.fullmatch(config.lora_modules, m_name):\n",
    "            for c_name, layer in dict(module.named_children()).items():\n",
    "                if re.fullmatch(config.lora_layers, c_name):\n",
    "                    assert isinstance(\n",
    "                        layer, nn.Linear\n",
    "                    ), f\"LoRA can only be applied to torch.nn.Linear, but {layer} is {type(layer)}.\"\n",
    "                    setattr(\n",
    "                        module,\n",
    "                        c_name,\n",
    "                        LoRALinear(layer, config.lora_rank, config.lora_scaling_rank, config.lora_init_scale),\n",
    "                    )\n",
    "    return transformer class ClassConfig:\n",
    "    def __init__(self, dropout=0.2, num_labels=1):\n",
    "        self.dropout_rate = dropout\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "class T5EncoderClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config, class_config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(class_config.dropout_rate)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, class_config.num_labels)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "\n",
    "        hidden_states =  torch.mean(hidden_states,dim=1)  # avg embedding\n",
    "\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = torch.tanh(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.out_proj(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class T5EncoderForSimpleSequenceClassification(T5PreTrainedModel):\n",
    "\n",
    "    def __init__(self, config: T5Config, class_config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = class_config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n",
    "\n",
    "        encoder_config = copy.deepcopy(config)\n",
    "        encoder_config.use_cache = False\n",
    "        encoder_config.is_encoder_decoder = False\n",
    "        self.encoder = T5Stack(encoder_config, self.shared)\n",
    "\n",
    "        self.dropout = nn.Dropout(class_config.dropout_rate) \n",
    "        self.classifier = T5EncoderClassificationHead(config, class_config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "\n",
    "    def parallelize(self, device_map=None):\n",
    "        self.device_map = (\n",
    "            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n",
    "            if device_map is None\n",
    "            else device_map\n",
    "        )\n",
    "        assert_device_map(self.device_map, len(self.encoder.block))\n",
    "        self.encoder.parallelize(self.device_map)\n",
    "        self.classifier = self.classifier.to(self.encoder.first_device)\n",
    "        self.model_parallel = True\n",
    "\n",
    "    def deparallelize(self):\n",
    "        self.encoder.deparallelize()\n",
    "        self.encoder = self.encoder.to(\"cpu\")\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.shared\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.shared = new_embeddings\n",
    "        self.encoder.set_input_embeddings(new_embeddings)\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        ) def PT5_classification_model(num_labels, half_precision):\n",
    "    # Load PT5 and tokenizer\n",
    "    # possible to load the half preciion model (thanks to @pawel-rezo for pointing that out)\n",
    "    if not half_precision:\n",
    "        model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "    elif half_precision and torch.cuda.is_available() : \n",
    "        tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "        model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\", torch_dtype=torch.float16).to(torch.device('cuda'))\n",
    "    else:\n",
    "          raise ValueError('Half precision can be run on GPU only.')\n",
    "    \n",
    "    # Create new Classifier model with PT5 dimensions\n",
    "    class_config=ClassConfig(num_labels=num_labels)\n",
    "    class_model=T5EncoderForSimpleSequenceClassification(model.config,class_config)\n",
    "    \n",
    "    # Set encoder and embedding weights to checkpoint weights\n",
    "    class_model.shared=model.shared\n",
    "    class_model.encoder=model.encoder    \n",
    "    \n",
    "    # Delete the checkpoint model\n",
    "    model=class_model\n",
    "    del class_model\n",
    "    \n",
    "    # Print number of trainable parameters\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    print(\"ProtT5_Classfier\\nTrainable Parameter: \"+ str(params))    \n",
    " \n",
    "    # Add model modification lora\n",
    "    config = LoRAConfig()\n",
    "    \n",
    "    # Add LoRA layers\n",
    "    model = modify_with_lora(model, config)\n",
    "    \n",
    "    # Freeze Embeddings and Encoder (except LoRA)\n",
    "    for (param_name, param) in model.shared.named_parameters():\n",
    "                param.requires_grad = False\n",
    "    for (param_name, param) in model.encoder.named_parameters():\n",
    "                param.requires_grad = False       \n",
    "\n",
    "    for (param_name, param) in model.named_parameters():\n",
    "            if re.fullmatch(config.trainable_param_names, param_name):\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # Print trainable Parameter          \n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    print(\"ProtT5_LoRA_Classfier\\nTrainable Parameter: \"+ str(params) + \"\\n\")\n",
    "    \n",
    "    return model, tokenizer # Deepspeed config for optimizer CPU offload\n",
    "\n",
    "ds_config = {\n",
    "    \"fp16\": {\n",
    "        \"enabled\": \"auto\",\n",
    "        \"loss_scale\": 0,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"initial_scale_power\": 16,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \"warmup_num_steps\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 2e8,\n",
    "        \"overlap_comm\": True,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 2e8,\n",
    "        \"contiguous_gradients\": True\n",
    "    },\n",
    "\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"steps_per_print\": 2000,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"wall_clock_breakdown\": False\n",
    "} # Set random seeds for reproducibility of your trainings run\n",
    "def set_seeds(s):\n",
    "    torch.manual_seed(s)\n",
    "    np.random.seed(s)\n",
    "    random.seed(s)\n",
    "    set_seed(s)\n",
    "\n",
    "# Dataset creation\n",
    "def create_dataset(tokenizer,seqs,labels):\n",
    "    tokenized = tokenizer(seqs, max_length=1024, padding=False, truncation=True)\n",
    "    dataset = Dataset.from_dict(tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "\n",
    "    return dataset\n",
    "    \n",
    "# Main training fuction\n",
    "def train_per_protein(\n",
    "        train_df,         #training data\n",
    "        valid_df,         #validation data      \n",
    "        num_labels= 1,    #1 for regression, >1 for classification\n",
    "    \n",
    "        # effective training batch size is batch * accum\n",
    "        # we recommend an effective batch size of 8 \n",
    "        batch= 4,         #for training\n",
    "        accum= 2,         #gradient accumulation\n",
    "    \n",
    "        val_batch = 16,   #batch size for evaluation\n",
    "        epochs= 10,       #training epochs\n",
    "        lr= 3e-4,         #recommended learning rate\n",
    "        seed= 42,         #random seed\n",
    "        deepspeed= True,  #if gpu is large enough disable deepspeed for training speedup\n",
    "        mixed= False,     #enable mixed precision training\n",
    "        gpu= 1 ):         #gpu selection (1 for first gpu)\n",
    "\n",
    "    # Set gpu device\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu-1)\n",
    "    \n",
    "    # Set all random seeds\n",
    "    set_seeds(seed)\n",
    "    \n",
    "    # load model\n",
    "    model, tokenizer = PT5_classification_model(num_labels=num_labels)\n",
    "\n",
    "    # Preprocess inputs\n",
    "    # Replace uncommon AAs with \"X\"\n",
    "    train_df[\"sequence\"]=train_df[\"sequence\"].str.replace('|'.join([\"O\",\"B\",\"U\",\"Z\"]),\"X\",regex=True)\n",
    "    valid_df[\"sequence\"]=valid_df[\"sequence\"].str.replace('|'.join([\"O\",\"B\",\"U\",\"Z\"]),\"X\",regex=True)\n",
    "    # Add spaces between each amino acid for PT5 to correctly use them\n",
    "    train_df['sequence']=train_df.apply(lambda row : \" \".join(row[\"sequence\"]), axis = 1)\n",
    "    valid_df['sequence']=valid_df.apply(lambda row : \" \".join(row[\"sequence\"]), axis = 1)\n",
    "\n",
    "    # Create Datasets\n",
    "    train_set=create_dataset(tokenizer,list(train_df['sequence']),list(train_df['label']))\n",
    "    valid_set=create_dataset(tokenizer,list(valid_df['sequence']),list(valid_df['label']))\n",
    "\n",
    "    # Huggingface Trainer arguments\n",
    "    args = TrainingArguments(\n",
    "        \"./\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        logging_strategy = \"epoch\",\n",
    "        save_strategy = \"no\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch,\n",
    "        per_device_eval_batch_size=val_batch,\n",
    "        gradient_accumulation_steps=accum,\n",
    "        num_train_epochs=epochs,\n",
    "        seed = seed,\n",
    "        deepspeed= ds_config if deepspeed else None,\n",
    "        fp16 = mixed,\n",
    "    ) \n",
    "\n",
    "    # Metric definition for validation data\n",
    "    def compute_metrics(eval_pred):\n",
    "        if num_labels>1:  # for classification\n",
    "            metric = load(\"accuracy\")\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "        else:  # for regression\n",
    "            metric = load(\"spearmanr\")\n",
    "            predictions, labels = eval_pred\n",
    "\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    # Trainer          \n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=valid_set,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )    \n",
    "    \n",
    "    # Train model\n",
    "    trainer.train()\n",
    "\n",
    "    return tokenizer, model, trainer.state.log_history tokenizer, model, history = train_per_protein(my_train, my_valid, num_labels=1, batch=1, accum=8, epochs=20, seed=42) # Get loss, val_loss, and the computed metric from history\n",
    "loss = [x['loss'] for x in history if 'loss' in x]\n",
    "val_loss = [x['eval_loss'] for x in history if 'eval_loss' in x]\n",
    "\n",
    "# Get spearman (for regression) or accuracy value (for classification)\n",
    "if [x['eval_spearmanr'] for x in history if 'eval_spearmanr' in x] != []:\n",
    "    metric = [x['eval_spearmanr'] for x in history if 'eval_spearmanr' in x]\n",
    "else:\n",
    "    metric = [x['eval_accuracy'] for x in history if 'eval_accuracy' in x]\n",
    "\n",
    "epochs = [x['epoch'] for x in history if 'loss' in x]\n",
    "\n",
    "# Create a figure with two y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot loss and val_loss on the first y-axis\n",
    "line1 = ax1.plot(epochs, loss, label='train_loss')\n",
    "line2 = ax1.plot(epochs, val_loss, label='val_loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "\n",
    "# Plot the computed metric on the second y-axis\n",
    "line3 = ax2.plot(epochs, metric, color='red', label='val_metric')\n",
    "ax2.set_ylabel('Metric')\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "# Combine the lines from both y-axes and create a single legend\n",
    "lines = line1 + line2 + line3\n",
    "labels = [line.get_label() for line in lines]\n",
    "ax1.legend(lines, labels, loc='lower left')\n",
    "\n",
    "# Show the plot\n",
    "plt.title(\"Training History\")\n",
    "plt.show() def save_model(model,filepath):\n",
    "# Saves all parameters that were changed during finetuning\n",
    "\n",
    "    # Create a dictionary to hold the non-frozen parameters\n",
    "    non_frozen_params = {}\n",
    "\n",
    "    # Iterate through all the model parameters\n",
    "    for param_name, param in model.named_parameters():\n",
    "        # If the parameter has requires_grad=True, add it to the dictionary\n",
    "        if param.requires_grad:\n",
    "            non_frozen_params[param_name] = param\n",
    "\n",
    "    # Save only the finetuned parameters \n",
    "    torch.save(non_frozen_params, filepath)\n",
    "\n",
    "    \n",
    "def load_model(filepath, num_labels=1, mixed = False):\n",
    "# Creates a new PT5 model and loads the finetuned weights from a file\n",
    "\n",
    "    # load a new model\n",
    "    model, tokenizer = PT5_classification_model(num_labels=num_labels, half_precision=mixed)\n",
    "    \n",
    "    # Load the non-frozen parameters from the saved file\n",
    "    non_frozen_params = torch.load(filepath)\n",
    "\n",
    "    # Assign the non-frozen parameters to the corresponding parameters of the model\n",
    "    for param_name, param in model.named_parameters():\n",
    "        if param_name in non_frozen_params:\n",
    "            param.data = non_frozen_params[param_name].data\n",
    "\n",
    "    return tokenizer, model tokenizer, model_reload = load_model(\"./PT5_GB1_finetuned.pth\", num_labels=1, mixed=False) # Put both models to the same device\n",
    "model=model.to(\"cpu\")\n",
    "model_reload=model_reload.to(\"cpu\")\n",
    "\n",
    "# Iterate through the parameters of the two models and compare the data\n",
    "for param1, param2 in zip(model.parameters(), model_reload.parameters()):\n",
    "    if not torch.equal(param1.data, param2.data):\n",
    "        print(\"Models have different weights\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Models have identical weights\") # For this we import the \"three_vs_rest\" GB1 dataset again \n",
    "# from https://github.com/J-SNACKKB/FLIP\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "# Download the zip file from GitHub\n",
    "url = 'https://github.com/J-SNACKKB/FLIP/raw/main/splits/gb1/splits.zip'\n",
    "response = requests.get(url)\n",
    "zip_file = zipfile.ZipFile(BytesIO(response.content))\n",
    "\n",
    "# Load the `three_vs_rest.csv` file into a pandas dataframe\n",
    "with zip_file.open('splits/three_vs_rest.csv') as file:\n",
    "    df = pd.read_csv(file) # Select only test data\n",
    "my_test=df[df.set==\"test\"]\n",
    "\n",
    "# Set column names to \"sequence\" and \"label\"\n",
    "my_test.columns=[\"sequence\",\"label\"]+list(my_test.columns[2:])\n",
    "\n",
    "# Drop unneeded columns\n",
    "my_test=my_test[[\"sequence\",\"label\"]]\n",
    "print(my_test.head(5))\n",
    "\n",
    "# Preprocess sequences\n",
    "my_test[\"sequence\"]=my_test[\"sequence\"].str.replace('|'.join([\"O\",\"B\",\"U\",\"Z\"]),\"X\",regex=True)\n",
    "my_test['sequence']=my_test.apply(lambda row : \" \".join(row[\"sequence\"]), axis = 1) #Use reloaded model\n",
    "model = model_reload\n",
    "del model_reload\n",
    "\n",
    "# Set the device to use\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# create Dataset\n",
    "test_set=create_dataset(tokenizer,list(my_test['sequence']),list(my_test['label']))\n",
    "# make compatible with torch DataLoader\n",
    "test_set = test_set.with_format(\"torch\", device=device)\n",
    "\n",
    "# Create a dataloader for the test dataset\n",
    "test_dataloader = DataLoader(test_set, batch_size=16, shuffle=False)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #add batch results(logits) to predictions\n",
    "        predictions += model(input_ids, attention_mask=attention_mask).logits.tolist() # Regression\n",
    "print(stats.spearmanr(a=predictions, b=my_test.label, axis=0))\n",
    "\n",
    "# Classification\n",
    "# we need to determine the prediction class from the logit output\n",
    "# predictions= [item.argmax() for item in np.array(predictions)]\n",
    "# print(\"Accuracy: \", accuracy_score(my_test.label, predictions)) :_:: pleaase update the whole code to run kaggle notebook t4."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNhKinP9loPRXMd91St0csY",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
